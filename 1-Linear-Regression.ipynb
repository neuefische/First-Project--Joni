{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression\n",
    "What is linear regression?\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/400px-Linear_regression.svg.png\" height=\"500\"/></center>\n",
    "\n",
    "In statistics, **linear regression** is an approach for modeling the relationship between a response variable $y$ and one or more explanatory variables.  \n",
    "Important: Linear regression can prove a relationship, but it **cannot prove causality**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Simple linear regresssion**: only one explanatory variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Multiple linear regression**: more than one explanatory variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's start with an example\n",
    "\n",
    "Suppose we want to explore the relationshop between the fuel efficiency of a car (mpg) using its weight.\n",
    "\n",
    "We will use the data set `cars` which contains variables `mpg` and `weight`.\n",
    "This dataset is downloaded from https://vincentarelbundock.github.io/Rdatasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T09:49:25.659140Z",
     "start_time": "2020-02-05T09:49:24.271595Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T09:49:26.361132Z",
     "start_time": "2020-02-05T09:49:26.084862Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cars = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv\")\n",
    "cars = cars.rename(columns={'Unnamed: 0':'car_model',\n",
    "                            'wt':\"weight\"});\n",
    "cars.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "We want to model the relationship between `mpg` and `weight` with a straight line.\n",
    "\n",
    "First, let's look at a scatterplot to get an idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T09:49:26.978524Z",
     "start_time": "2020-02-05T09:49:26.812838Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the variables of interest first\n",
    "cars.plot(x='weight', \n",
    "          y='mpg', \n",
    "          kind='scatter');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is your guess for intercept and slope?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is Linear regression?\n",
    "----\n",
    "\n",
    "Linear regression is just the fancy term for finding the line of best fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In other words, we are looking for the slope and intercept that defines a line that fits the data as well as possible \n",
    "\n",
    "    **Intercept** - The value for $y$ when $x=0$ \n",
    "\n",
    "    **slope** - For each unit increase in $x$, the expected increase/decrease in $y$ (in the case of multiple linear regression, we need to add \"holding all other explanatory variables \n",
    "    constant\", since there are more than one exaplanatory variable in the model.)  \n",
    "    \n",
    "* 'As well as possible' often means that we are trying to minimize the sum of squared residuals\n",
    "\n",
    "* Below we will write a function which will find the line between two points\n",
    "\n",
    "![](https://www.mathsisfun.com/algebra/images/graph-2-points-b.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T09:49:27.892716Z",
     "start_time": "2020-02-05T09:49:27.883025Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_line_equation(p1, p2):\n",
    "    \"\"\"\n",
    "    Solve the system of equations:\n",
    "    y1 = m*x1 + b\n",
    "    y2 = m*x2 + b\n",
    "    \n",
    "    This translates to:\n",
    "    m = (y2 - y1) / (x2 - x1)\n",
    "    b = y1 - m*x1\n",
    "    \n",
    "    Input:\n",
    "    p1: first point [x1, y1]\n",
    "    p2: second point [x2, y2]\n",
    "    \n",
    "    returns: slope, intercept\n",
    "    \"\"\"\n",
    "    m = (p2[1] - p1[1]) / (p2[0] - p1[0]) # Slope\n",
    "    b = p1[1] - m * p1[0] # Intercept\n",
    "    return  m, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T09:49:27.892716Z",
     "start_time": "2020-02-05T09:49:27.883025Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the intercept and slope of a line between two points\n",
    "car1 = [cars.weight[1],cars.mpg[1]]\n",
    "car2 = [cars.weight[2],cars.mpg[2]]\n",
    "slope, intercept = get_line_equation(p1=car1, p2=car2) \n",
    "print('intercept: ', intercept)\n",
    "print('slope: ', slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T09:49:28.223540Z",
     "start_time": "2020-02-05T09:49:28.081039Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot line fitted on two points versus on all of the points\n",
    "fig = cars.plot(x='weight', \n",
    "                y='mpg', \n",
    "                kind='scatter')\n",
    "# plot the linear regression line\n",
    "plt.plot(np.unique(cars.weight), np.poly1d(np.polyfit(cars.weight, cars.mpg, 1))(np.unique(cars.weight)));\n",
    "# plot the line between two points\n",
    "plt.plot([car1[0], car2[0]], [car1[1], car2[1]], color=\"red\", linewidth=4)\n",
    "plt.title(\"Line fitted on two points versus on all of the points\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your understanding:  \n",
    "Are the slope of the red and the blue lines the same?  \n",
    "Why are the lines not overlapping?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "------\n",
    "## Linear Regression: Formally Defined\n",
    "\n",
    "\n",
    "Genral formula for simple linear regression:\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$$  \n",
    "$$\\text{for i = } 1, \\dots, n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression terms\n",
    "\n",
    "\n",
    "$\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$, where $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are the estimated intercept and slope of the line\n",
    "\n",
    "$\\hat{y}$ are the points that fall on the fitted straight line "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data is modeled as Fit + Residual\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$$  \n",
    "$$\\text{for i = } 1, \\dots, n$$\n",
    "\n",
    "A **residual** is the difference between an observed value and the fitted value provided by a model, $e_i = y_i - \\hat{y}_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"http://68.media.tumblr.com/7efec0c9403dd3b9eeb181e1681cd3d2/tumblr_inline_nii9j9oqS61rs8rb9.gif\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is the biggest assumption of LINEAR Regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Model assumption: y and x are linearly related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ordinary Least Squares (OLS): Don't discount that is called \"ordinary\"\n",
    "\n",
    "OLS is the simplest and most common way to estimate the intercept and the slope\n",
    "\n",
    "It estimates by minimizing the sum of squared residuals:$\\sum_{i=1}^n e_i^2$\n",
    "\n",
    "[Visual explanation](http://setosa.io/ev/ordinary-least-squares-regression/) for better understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "-----\n",
    "## Simple linear regression in practice\n",
    "In the next section you will build a Linear Regression model using one dependent and one independent variable from the cars dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Therefore, we need to solve the equation for $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$: \n",
    "\n",
    "$\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}$, or\n",
    "\n",
    "$\\hat{\\beta}_1 = r_{xy} \\frac{s_y}{s_x}$, where $r_{xy}$ is the correlation between $x$ and $y$, $s_x$ and $s_y$ are the standard deviations of $x$ and $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T09:49:36.328583Z",
     "start_time": "2020-02-05T09:49:35.909214Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T09:49:36.358578Z",
     "start_time": "2020-02-05T09:49:36.342791Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "# Choose the predictor and add a constant term\n",
    "X = cars[['weight']]\n",
    "y = cars.mpg\n",
    "# Our model needs an intercept so we add a column of 1s:\n",
    "X = sms.add_constant(X)\n",
    "display(X.head())\n",
    "display(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T09:49:49.416173Z",
     "start_time": "2020-02-05T09:49:49.396289Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create an OLS model\n",
    "model = sms.OLS(y, X)\n",
    "# use the data to calculate the intercept and slope\n",
    "results = model.fit()\n",
    "# return the output of the model\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Interpretation\n",
    "For a complete explanation of all of the metrics within the model summary check the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**const coef** - This is the intercept, in other words: value of y when x = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**weight coef** - This is the slope, in other words: Amount, y changes for each unit change of x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\textbf{R}^2$ - Proportion of the variation in $y$ that is explained by the model. Measured on a scale from 0 (bad) to 1 (good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deep-dive into $\\textbf{R}^2$ \n",
    "\n",
    "How to calculate $\\textbf{R}^2$:\n",
    "\n",
    "$R^2 = \\frac{SSR}{SST}$\n",
    "\n",
    "SST = SSR + SSE\n",
    "\n",
    "* Total sum of squares (**SST**): $\\sum_{i=1}^n (y_i - \\bar{y})^2$\n",
    "\n",
    "\n",
    "* Regression sum of squares (**SSR**): $\\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2$\n",
    "\n",
    "\n",
    "* Residual sum of squares (**SSE**): $\\sum_{i=1}^n e_i^2$  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T09:49:52.701009Z",
     "start_time": "2020-02-05T09:49:52.615703Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept, slope = results.params\n",
    "intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results of our model\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "x = cars.weight\n",
    "# add data points\n",
    "ax.scatter(x, y, alpha=0.5, color='orchid')\n",
    "fig.suptitle('Relationship between weight and mpg')\n",
    "# plotting regression line\n",
    "ax.plot(x, x*slope +intercept, '-', color='darkorchid', linewidth=2);\n",
    "ax.set_ylabel(\"mpg\");\n",
    "ax.set_xlabel(\"weight\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Try it out yourself\n",
    "\n",
    "We added a constant earlier to our linear regression model. \n",
    "What happens if we don't use one? Is our model describing the dependent variable better or worse?\n",
    "\n",
    "Try out at least one other independent variable to explain `mpg` with a simple linear regression.\n",
    "Also plot the points and the regression line.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "\n",
    "- Linear regression is finding the best fit line to data\n",
    "- Simple linear regression is 1 outcome and 1 explanatory variable\n",
    "- Linear regression can prove a relationship, but it **cannot prove causality**\n",
    "- Ordinary Least Squares (OLS) is the fitting algorithm, which mathematically finds the best solution\n",
    "- $\\textbf{R}^2$ is the proportion of the variation explained by the model\n",
    "\n",
    "GOOD RESOURCE: https://realpython.com/linear-regression-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Model Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can find the R-Squared, which is 0.95 i.e. very highly related\n",
    "* You can also look at the coefficients of the model for intercept and slope (next to \"height\")\n",
    "* Kurtosis and Skew values are shown here\n",
    "* A lot of significance testing is being done here\n",
    "\n",
    "\n",
    "**Here is a brief description of these measures:**\n",
    "\n",
    "The left part of the first table gives some specifics on the data and the model:\n",
    "\n",
    "* **Dep. Variable**: Singular. Which variable is the point of interest of the model\n",
    "* **Model**: Technique used, an abbreviated version of Method (see methods for more).\n",
    "* **Method**: The loss function optimized in the parameter selection process. Least Squares since it picks the parameters that reduce the training error. This is also known as Mean Square Error [MSE].\n",
    "* **No. Observations**: The number of observations used by the model, or size of the training data.\n",
    "* **Degrees of Freedom Residuals**: Degrees of freedom of the residuals, which is the number of observations – number of parameters. Intercept is a parameter. The purpose of Degrees of Freedom is to reflect the impact of descriptive/summarizing statistics in the model, which in regression is the coefficient. Since the observations must \"live up\" to these parameters, they only have so many free observations, and the rest must be reserved to \"live up\" to the parameters' prophecy. This internal mechanism ensures that there are enough observations to match the parameters.\n",
    "* **Degrees of Freedom Model**: The number of parameters in the model (not including the constant/intercept term if present)\n",
    "* **Covariance Type**: Robust regression methods are designed to be not overly affected by violations of assumptions by the underlying data-generating process. Since this model is Ordinary Least Squares, it is non-robust and therefore highly sensitive to outliers.\n",
    "\n",
    "The right part of the first table shows the goodness of fit \n",
    "\n",
    "* **R-squared**: The coefficient of determination, the Sum Squares of Regression divided by Total Sum Squares. This translates to the percent of variance explained by the model. The remaining percentage represents the variance explained by error, the E term, the part that model and predictors fail to grasp.\n",
    "* **Adj. R-squared**: Version of the R-Squared that penalizes additional independent variables. \n",
    "* **F-statistic**: A measure of how significant the fit is. The mean squared error of the model divided by the mean squared error of the residuals. Feeds into the calculation of the P-Value.\n",
    "* **Prob (F-statistic) or P-Value**: The probability that a sample like this would yield the above statistic, and whether the model's verdict on the null hypothesis will consistently represent the population. Does not measure effect magnitude, instead measures the integrity and consistency of this test on this group of data.\n",
    "* **Log-likelihood**: The log of the likelihood function.\n",
    "* **AIC**: The Akaike Information Criterion. Adjusts the log-likelihood based on the number of observations and the complexity of the model. Penalizes the model selection metrics when more independent variables are added.\n",
    "* **BIC**: The Bayesian Information Criterion. Similar to the AIC, but has a higher penalty for models with more parameters. Penalizes the model selection metrics when more independent variables are added.\n",
    "\n",
    "Second Table: Coefficient Reports \n",
    "\n",
    "* **coef**: The estimated value of the coefficient. By how much the model multiplies the independent value by.\n",
    "* **std err**: The basic standard error of the estimate of the coefficient. Average distance deviation of the points from the model, which offers a unit relevant way to gauge model accuracy.\n",
    "* **t**: The t-statistic value. This is a measure of how statistically significant the coefficient is.\n",
    "* **P > |t|**: P-value that the null-hypothesis that the coefficient = 0 is true. If it is less than the confidence level, often 0.05, it indicates that there is a statistically significant relationship between the term and the response.\n",
    "* **[95.0% Conf. Interval]**: The lower and upper values of the 95% confidence interval. Specific range of the possible coefficient values.\n",
    "\n",
    "Third Table: Residuals, Autocorrelation, and Multicollinearity \n",
    "\n",
    "* **Skewness**: A measure of the symmetry of the data about the mean. Normally-distributed errors should be symmetrically distributed about the mean (equal amounts above and below the line). The normal distribution has 0 skew.\n",
    "* **Kurtosis**: A measure of the shape of the distribution. Compares the amount of data close to the mean with those far away from the mean (in the tails), so model \"peakiness\". The normal distribution has a Kurtosis of 3, and the greater the number, the more the curve peaks.\n",
    "* **Omnibus D’Angostino’s test**: It provides a combined statistical test for the presence of skewness and kurtosis.\n",
    "* **Prob(Omnibus)**: The above statistic turned into a probability\n",
    "* **Jarque-Bera**: A different test of the skewness and kurtosis\n",
    "* **Prob (JB)**: The above statistic turned into a probability\n",
    "* **Durbin-Watson**: A test for the presence of autocorrelation (that the errors are not independent), which is often important in time-series analysis\n",
    "* **Cond. No**: A test for multicollinearity (if in a fit with multiple parameters, the parameters are related to each other).\n",
    "\n",
    "The interpretation of some of these measures will be explained in the next lessons. For others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
